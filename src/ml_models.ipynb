{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _CYME(df: pd.DataFrame) -> float:\n",
    "    \"\"\" Compute the CYME metric, that is 1/2(median(yearly error) + median(monthly error))\"\"\"\n",
    "\n",
    "    yearly_agg = df.groupby(\"cluster_nl\")[[\"target\", \"prediction\"]].sum().reset_index()\n",
    "    yearly_error = abs((yearly_agg[\"target\"] - yearly_agg[\"prediction\"])/yearly_agg[\"target\"]).median()\n",
    "\n",
    "    monthly_error = abs((df[\"target\"] - df[\"prediction\"])/df[\"target\"]).median()\n",
    "\n",
    "    return 1/2*(yearly_error + monthly_error)\n",
    "\n",
    "\n",
    "def _metric(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute metric of submission.\n",
    "\n",
    "    :param df: Dataframe with target and 'prediction', and identifiers.\n",
    "    :return: Performance metric\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Split 0 actuals - rest\n",
    "    zeros = df[df[\"zero_actuals\"] == 1]\n",
    "    recent = df[df[\"zero_actuals\"] == 0]\n",
    "\n",
    "    # weight for each group\n",
    "    zeros_weight = len(zeros)/len(df)\n",
    "    recent_weight = 1 - zeros_weight\n",
    "\n",
    "    # Compute CYME for each group\n",
    "    return round(recent_weight*_CYME(recent) + zeros_weight*min(1,_CYME(zeros)),8)\n",
    "\n",
    "\n",
    "def compute_metric(submission: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Compute metric.\n",
    "\n",
    "    :param submission: Prediction. Requires columns: ['cluster_nl', 'date', 'target', 'prediction']\n",
    "    :return: Performance metric.\n",
    "    \"\"\"\n",
    "\n",
    "    submission[\"date\"] = pd.to_datetime(submission[\"date\"])\n",
    "    submission = submission[['cluster_nl', 'date', 'target', 'prediction', 'zero_actuals']]\n",
    "\n",
    "    return _metric(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset directory:\n",
      "/home/ferrandf/novartis-datathon\n",
      "['submission_data.csv', 'train_data.csv', 'First_Clean_train_data.csv', 'train_data_TRY1.csv']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118906 entries, 0 to 118905\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   brand               118906 non-null  object \n",
      " 1   che_pc_usd          118906 non-null  float64\n",
      " 2   che_perc_gdp        118906 non-null  float64\n",
      " 3   cluster_nl          118906 non-null  object \n",
      " 4   corporation         118906 non-null  object \n",
      " 5   country             118906 non-null  object \n",
      " 6   launch_date         118906 non-null  object \n",
      " 7   date                118906 non-null  object \n",
      " 8   drug_id             118906 non-null  object \n",
      " 9   ind_launch_date     118906 non-null  object \n",
      " 10  indication          118906 non-null  object \n",
      " 11  insurance_perc_che  118906 non-null  float64\n",
      " 12  population          118906 non-null  float64\n",
      " 13  prev_perc           118906 non-null  float64\n",
      " 14  price_month         118906 non-null  float64\n",
      " 15  price_unit          118906 non-null  float64\n",
      " 16  public_perc_che     118906 non-null  float64\n",
      " 17  therapeutic_area    118906 non-null  object \n",
      " 18  target              118906 non-null  float64\n",
      "dtypes: float64(9), object(10)\n",
      "memory usage: 17.2+ MB\n",
      "Training data info: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118906 entries, 0 to 118905\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   brand               118906 non-null  object \n",
      " 1   che_pc_usd          118906 non-null  float64\n",
      " 2   che_perc_gdp        118906 non-null  float64\n",
      " 3   cluster_nl          118906 non-null  object \n",
      " 4   corporation         118906 non-null  object \n",
      " 5   country             118906 non-null  object \n",
      " 6   launch_date         118906 non-null  object \n",
      " 7   date                118906 non-null  object \n",
      " 8   drug_id             118906 non-null  object \n",
      " 9   ind_launch_date     118906 non-null  object \n",
      " 10  indication          118906 non-null  object \n",
      " 11  insurance_perc_che  118906 non-null  float64\n",
      " 12  population          118906 non-null  float64\n",
      " 13  prev_perc           118906 non-null  float64\n",
      " 14  price_month         118906 non-null  float64\n",
      " 15  price_unit          118906 non-null  float64\n",
      " 16  public_perc_che     118906 non-null  float64\n",
      " 17  therapeutic_area    118906 non-null  object \n",
      " 18  target              118906 non-null  float64\n",
      "dtypes: float64(9), object(10)\n",
      "memory usage: 87.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "input_path = \"dataset\"\n",
    "print(\"Files in dataset directory:\")\n",
    "print(os.path.dirname(os.getcwd()))\n",
    "print(os.listdir(os.path.join(os.path.dirname(os.getcwd()), input_path)))\n",
    "\n",
    "features_cols = [\n",
    "    \"brand\", \n",
    "    \"che_pc_usd\", \n",
    "    \"che_perc_gdp\", \n",
    "    \"corporation\", \n",
    "    \"country\", \n",
    "    \"launch_date\", \n",
    "    \"drug_id\", \n",
    "    \"ind_launch_date\", \n",
    "    \"indication\", \n",
    "    \"insurance_perc_che\", \n",
    "    \"population\", \n",
    "    \"prev_perc\", \n",
    "    \"price_month\", \n",
    "    \"price_unit\", \n",
    "    \"public_perc_che\", \n",
    "    \"therapeutic_area\",\n",
    "]\n",
    "target_col = \"target\"\n",
    "id_col = [\"cluster_nl\",\"date\"]\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(os.getcwd()), input_path)\n",
    "# Load datasets\n",
    "# data = pd.read_csv(f\"{base_dir}/train_data.csv\", usecols=features_cols + [target_col] + id_col)\n",
    "data = pd.read_csv(f\"{base_dir}/train_data_TRY1.csv\", usecols=features_cols + [target_col] + id_col)\n",
    "test_data = pd.read_csv(f\"{base_dir}/submission_data.csv\", usecols=features_cols + id_col)\n",
    "\n",
    "y = data[target_col]\n",
    "\n",
    "# Display dataset informations\n",
    "print(f\"Training data info: {data.info()}\")\n",
    "print(data.info(memory_usage=\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: Index(['che_pc_usd', 'che_perc_gdp', 'insurance_perc_che', 'population',\n",
      "       'prev_perc', 'price_month', 'price_unit', 'public_perc_che'],\n",
      "      dtype='object')\n",
      "Categorical features: Index(['brand', 'cluster_nl', 'corporation', 'country', 'launch_date', 'date',\n",
      "       'drug_id', 'ind_launch_date', 'indication', 'therapeutic_area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Separate numeric and categorical features for imputation\n",
    "numeric_features = data.select_dtypes(include=['float64']).drop(columns=[target_col], errors='ignore').columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.72931342  0.1093513   0.84690232 ...  0.06633365 -0.21597011\n",
      "   0.24331455]\n",
      " [-5.76038552 -4.8459021  -1.9239651  ... -1.35219885  0.11488193\n",
      "  -9.10311876]\n",
      " [-0.72931342  0.1093513   0.84690232 ... -1.35219885  0.93443557\n",
      "   0.24331455]\n",
      " ...\n",
      " [-0.91518872 -0.21468219  0.78305745 ...  0.07397957 -0.21352666\n",
      "   0.54315001]\n",
      " [ 0.78612113  0.76809541 -1.9239651  ...  0.74117745  0.58141327\n",
      "   0.73777625]\n",
      " [-0.91518872 -0.21468219  0.78305745 ... -1.35219885 -0.22195944\n",
      "   0.54315001]]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "X = data.drop(columns=[target_col]+id_col)\n",
    "X_test = test_data.drop(columns=id_col)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def preprocess_data(X, preprocessor=None, fit=True):\n",
    "    numerical_features = X.select_dtypes(include=['float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['category']).columns\n",
    "\n",
    "    if preprocessor is None:\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if fit:\n",
    "        X_transformed = preprocessor.fit_transform(X)\n",
    "    else:\n",
    "        X_transformed = preprocessor.transform(X)\n",
    "    \n",
    "    X_transformed = np.array(X_transformed)\n",
    "\n",
    "    return X_transformed, preprocessor\n",
    "\n",
    "# Preprocess data\n",
    "X_transformed, preprocessor = preprocess_data(X, fit=True)\n",
    "X_test_transformed, _ = preprocess_data(X_test, preprocessor=preprocessor, fit=False)\n",
    "\n",
    "print(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_transformed, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest100': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "# Evaluate models using cross-validation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "    results[name] = -scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23782 23782\n",
      "41674     1.000273\n",
      "108204    1.556673\n",
      "77650     2.509877\n",
      "36913     1.010462\n",
      "13619     1.011719\n",
      "            ...   \n",
      "42758     1.003945\n",
      "34976     1.017688\n",
      "41932     1.154981\n",
      "21926     1.053061\n",
      "96697     1.272071\n",
      "Name: target, Length: 23782, dtype: float64\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_valid)\n\u001b[1;32m     14\u001b[0m y_valid\u001b[38;5;241m.\u001b[39mdrop(y_valid\u001b[38;5;241m.\u001b[39mindex[:\u001b[38;5;241m1\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mvalidation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m y_valid\n\u001b[1;32m     20\u001b[0m validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid[features_cols])\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "validation = X_valid.copy()\n",
    "\n",
    "# Ensure y_valid is a pandas Series\n",
    "if not isinstance(y_valid, pd.Series):\n",
    "    y_valid = pd.Series(y_valid)\n",
    "print(len(y_valid), len(X_valid))\n",
    "# Check if the length of y_valid matches the number of rows in X_valid\n",
    "if len(y_valid) != len(X_valid):\n",
    "    \n",
    "    raise ValueError(\"Length of y_valid does not match the number of rows in X_valid\")\n",
    "\n",
    "print(y_valid)\n",
    "\n",
    "y_valid.drop(y_valid.index[:1], inplace=True)\n",
    "\n",
    "validation['target'] = y_valid\n",
    "\n",
    "\n",
    "\n",
    "validation['prediction'] = model.predict(X_valid[features_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally check performance\n",
    "print(\"Performance:\", compute_metric(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare submission\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m submission_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/submission_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/submission_template.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(submission_data[features_cols])\n",
      "File \u001b[0;32m~/novartis-datathon/novartis-venv/lib/python3.11/site-packages/pandas/io/parquet.py:651\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    500\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    654\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         )\n",
      "File \u001b[0;32m~/novartis-datathon/novartis-venv/lib/python3.11/site-packages/pandas/io/parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare submission\n",
    "submission_data = pd.read_parquet(f\"{base_dir}/submission_data.csv\")\n",
    "submission = pd.read_csv(f\"{base_dir}/submission_template.csv\")\n",
    "\n",
    "submission['prediction'] = model.predict(submission_data[features_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result file saved as 'result.csv'\n",
      "Submission file saved as 'submission.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest100</td>\n",
       "      <td>0.25852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model     RMSE\n",
       "0  RandomForest100  0.25852"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display RMSE results\n",
    "results_df = pd.DataFrame(results.items(), columns=['Model', 'RMSE']).sort_values(by='RMSE')\n",
    "results_df['RMSE'] = results_df['RMSE'].map(\"{:.5f}\".format)\n",
    "results_df\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "# Make predictions on the test set\n",
    "train_predictions = best_model.predict(X_test_transformed)\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    id_col[1]: pd.to_datetime(test_data[id_col[1]]).dt.strftime(\"%m/%d/%Y\"),\n",
    "    id_col[0]: test_data[id_col[0]],\n",
    "    # \"target\": data[target_col],\n",
    "    \"prediction\": train_predictions\n",
    "})\n",
    "\n",
    "result.to_csv('result.csv', index=False)\n",
    "print(\"Result file saved as 'result.csv'\")\n",
    "\n",
    "result.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: RandomForest\n"
     ]
    }
   ],
   "source": [
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_transformed, y)\n",
    "print(f\"Selected Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = np.expm1(model.predict(X_test_transformed))\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "    # In the provided CSV the longitude and latitude are inverted\n",
    "        \"Location.GIS.Latitude\": test_data[\"Location.GIS.Longitude\"].astype(float),\n",
    "        \"Location.GIS.Logitude\": test_data[\"Location.GIS.Latitude\"].astype(float),\n",
    "        id_col: test_data[id_col].astype(str),\n",
    "        target_col: test_predictions.astype(float)\n",
    "    })\n",
    "    \n",
    "    result_name = f\"result_{name}.csv\"\n",
    "    result.to_csv(result_name, index=False)\n",
    "    print(f\"Result file saved as '{result_name}'\")\n",
    "\n",
    "    submission_name = \"submission_{name}.csv\"\n",
    "    result[[id_col, target_col]].to_csv(submission_name, index=False)\n",
    "    print(f\"Submission file saved as '{submission_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result file saved as 'result_test.csv'\n",
      "Submission file saved as 'submission_test.csv'\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.expm1(best_model.predict(X_test_transformed))\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    # In the provided CSV the longitude and latitude are inverted\n",
    "    \"Location.GIS.Latitude\": test_data[\"Location.GIS.Longitude\"].astype(float),\n",
    "    \"Location.GIS.Logitude\": test_data[\"Location.GIS.Latitude\"].astype(float),\n",
    "    id_col: test_data[id_col],\n",
    "    target_col: test_predictions\n",
    "})\n",
    "\n",
    "result.to_csv('result_test.csv', index=False)\n",
    "print(\"Result file saved as 'result_test.csv'\")\n",
    "\n",
    "result[[id_col, target_col]].to_csv('submission_test.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_test.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novartis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
